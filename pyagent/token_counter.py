import tiktoken

class TokenCounter:
    """Tokenç»Ÿè®¡å™¨ï¼Œç”¨äºç»Ÿè®¡å¯¹è¯ä¸­çš„tokenä½¿ç”¨é‡"""
    
    def __init__(self, model_name: str = "qwen3-235b-a22b"):
        # æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
        if "qwen" in model_name.lower():
            self.encoding = tiktoken.get_encoding("o200k_base")  # Qwenä½¿ç”¨o200k_base
        else:
            self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        # å½“å‰è½®æ¬¡çš„tokenç»Ÿè®¡
        self.current_round_stats = {
            "user_input_tokens": 0,
            "llm_output_tokens": 0,  # åŒ…æ‹¬æ€ç»´é“¾ã€è‡ªç„¶è¯­è¨€ã€å·¥å…·å‚æ•°
            "tool_result_tokens": 0,
            "total_round_tokens": 0
        }
        
        # æ€»tokenç»Ÿè®¡
        self.total_stats = {
            "total_user_input_tokens": 0,
            "total_llm_output_tokens": 0,
            "total_tool_result_tokens": 0,
            "total_tokens": 0
        }
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        if not text:
            return 0
        return len(self.encoding.encode(text))
    
    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—æ¶ˆæ¯å¯¹è±¡çš„tokenæ•°é‡"""
        total_tokens = 0
        
        # å¤„ç†contentå­—æ®µï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œåˆ—è¡¨æ ¼å¼ï¼‰
        content = message.get("content")
        if content:
            if isinstance(content, str):
                total_tokens += self.count_tokens(content)
            elif isinstance(content, list):
                # å¤„ç†contentåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text" and item.get("text"):
                            total_tokens += self.count_tokens(item["text"])
                        elif item.get("type") == "image_url" and item.get("image_url", {}).get("url"):
                            # å›¾åƒURLçš„tokenä¼°ç®—ï¼ˆOpenAIçš„ä¼°ç®—æ–¹å¼ï¼‰
                            # åŸºç¡€token + å›¾åƒå¤§å°ä¼°ç®—
                            total_tokens += 85  # åŸºç¡€å›¾åƒtoken
                            
                            # å°è¯•ä»base64ä¼°ç®—å›¾åƒå¤§å°
                            url = item["image_url"]["url"]
                            if url.startswith("data:image"):
                                try:
                                    # æå–base64éƒ¨åˆ†
                                    base64_part = url.split(",")[1]
                                    # ä¼°ç®—tokenï¼ˆæ¯åƒç´ çº¦0.002 tokensï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                                    image_size = len(base64_part) * 0.75  # base64è§£ç åçš„å¤§å°
                                    if image_size > 100000:  # å¤§äº100KB
                                        total_tokens += 170  # é«˜åˆ†è¾¨ç‡å›¾åƒ
                                    elif image_size > 50000:  # å¤§äº50KB
                                        total_tokens += 85   # ä¸­ç­‰åˆ†è¾¨ç‡å›¾åƒ
                                except:
                                    total_tokens += 85  # é»˜è®¤ä¼°ç®—
        
        # è®¡ç®—thinkingçš„token
        if message.get("thinking"):
            total_tokens += self.count_tokens(str(message["thinking"]))
        
        # è®¡ç®—tool_callsçš„token
        if message.get("tool_calls"):
            for tool_call in message["tool_calls"]:
                if "function" in tool_call:
                    func_name = tool_call["function"].get("name", "")
                    func_args = tool_call["function"].get("arguments", "")
                    total_tokens += self.count_tokens(func_name) + self.count_tokens(func_args)
        
        return total_tokens
    
    def start_new_round(self):
        """å¼€å§‹æ–°ä¸€è½®å¯¹è¯ï¼Œé‡ç½®å½“å‰è½®æ¬¡ç»Ÿè®¡"""
        self.current_round_stats = {
            "user_input_tokens": 0,
            "llm_output_tokens": 0,
            "tool_result_tokens": 0,
            "total_round_tokens": 0
        }
    
    def add_user_input(self, text: str):
        """æ·»åŠ ç”¨æˆ·è¾“å…¥çš„tokenç»Ÿè®¡"""
        tokens = self.count_tokens(text)
        self.current_round_stats["user_input_tokens"] = tokens
        self.total_stats["total_user_input_tokens"] += tokens
    
    def add_llm_output(self, thinking: str = "", content: str = "", tool_calls: list = None):
        """æ·»åŠ LLMè¾“å‡ºçš„tokenç»Ÿè®¡"""
        tokens = 0
        
        # æ€ç»´é“¾token
        if thinking:
            tokens += self.count_tokens(thinking)
        
        # è‡ªç„¶è¯­è¨€å†…å®¹token
        if content:
            tokens += self.count_tokens(content)
        
        # å·¥å…·è°ƒç”¨token
        if tool_calls:
            for tool_call in tool_calls:
                if "function" in tool_call:
                    func_name = tool_call["function"].get("name", "")
                    func_args = tool_call["function"].get("arguments", "")
                    tokens += self.count_tokens(func_name) + self.count_tokens(func_args)
        
        self.current_round_stats["llm_output_tokens"] += tokens
        self.total_stats["total_llm_output_tokens"] += tokens
    
    def add_tool_result(self, result: str):
        """æ·»åŠ å·¥å…·è¿”å›ç»“æœçš„tokenç»Ÿè®¡"""
        tokens = self.count_tokens(str(result))
        self.current_round_stats["tool_result_tokens"] += tokens
        self.total_stats["total_tool_result_tokens"] += tokens
    
    def finish_round(self):
        """å®Œæˆå½“å‰è½®æ¬¡ï¼Œè®¡ç®—æ€»tokenæ•°"""
        self.current_round_stats["total_round_tokens"] = (
            self.current_round_stats["user_input_tokens"] +
            self.current_round_stats["llm_output_tokens"] +
            self.current_round_stats["tool_result_tokens"]
        )
        self.total_stats["total_tokens"] += self.current_round_stats["total_round_tokens"]
    
    def get_round_summary(self) -> str:
        """è·å–å½“å‰è½®æ¬¡çš„tokenç»Ÿè®¡æ‘˜è¦"""
        return f"""
ğŸ“Š å½“å‰è½®æ¬¡Tokenç»Ÿè®¡:
   ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {self.current_round_stats['user_input_tokens']} tokens
   ğŸ¤– LLMè¾“å‡º: {self.current_round_stats['llm_output_tokens']} tokens
   ğŸ”§ å·¥å…·ç»“æœ: {self.current_round_stats['tool_result_tokens']} tokens
   ğŸ“ˆ æœ¬è½®æ€»è®¡: {self.current_round_stats['total_round_tokens']} tokens

ğŸ“Š ç´¯è®¡Tokenç»Ÿè®¡:
   ğŸ‘¤ ç”¨æˆ·è¾“å…¥æ€»è®¡: {self.total_stats['total_user_input_tokens']} tokens
   ğŸ¤– LLMè¾“å‡ºæ€»è®¡: {self.total_stats['total_llm_output_tokens']} tokens
   ğŸ”§ å·¥å…·ç»“æœæ€»è®¡: {self.total_stats['total_tool_result_tokens']} tokens
   ğŸ“Š æ€»è®¡: {self.total_stats['total_tokens']} tokens
"""