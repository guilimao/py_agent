import tiktoken
import json

class TokenCounter:
    """Tokenç»Ÿè®¡å™¨ï¼Œç”¨äºç»Ÿè®¡å¯¹è¯ä¸­çš„tokenä½¿ç”¨é‡"""
    
    def __init__(self, model_name: str = "qwen3-235b-a22b"):
        # æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
        if "qwen" in model_name.lower():
            self.encoding = tiktoken.get_encoding("o200k_base")  # Qwenä½¿ç”¨o200k_base
        else:
            self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        # å½“å‰è½®æ¬¡çš„tokenç»Ÿè®¡
        self.current_round_stats = {
            "user_input_tokens": 0,
            "llm_output_tokens": 0,  # åŒ…æ‹¬æ€ç»´é“¾ã€è‡ªç„¶è¯­è¨€ã€å·¥å…·å‚æ•°
            "tool_result_tokens": 0,
            "total_round_tokens": 0
        }
        
        # æ€»tokenç»Ÿè®¡
        self.total_stats = {
            "total_input_tokens": 0,      # æ€»è¾“å…¥tokenï¼ˆåŒ…æ‹¬ç³»ç»Ÿæç¤ºã€å·¥å…·å®šä¹‰ã€ç”¨æˆ·è¾“å…¥ã€å·¥å…·è¿”å›ï¼‰
            "total_output_tokens": 0,     # æ€»è¾“å‡ºtokenï¼ˆLLMè¾“å‡ºï¼šæ€è€ƒ+è‡ªç„¶è¯­è¨€+å·¥å…·è°ƒç”¨ï¼‰
            "total_tokens": 0             # æ€»token = æ€»è¾“å…¥ + æ€»è¾“å‡º
        }
        
        # ç³»ç»Ÿåˆå§‹tokenç»Ÿè®¡
        self.initial_tokens = {
            "system_prompt_tokens": 0,
            "tools_definition_tokens": 0,
            "total_initial_tokens": 0
        }
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        if not text:
            return 0
        return len(self.encoding.encode(text))
    
    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—æ¶ˆæ¯å¯¹è±¡çš„tokenæ•°é‡"""
        total_tokens = 0
        
        # å¤„ç†contentå­—æ®µï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œåˆ—è¡¨æ ¼å¼ï¼‰
        content = message.get("content")
        if content:
            if isinstance(content, str):
                total_tokens += self.count_tokens(content)
            elif isinstance(content, list):
                # å¤„ç†contentåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text" and item.get("text"):
                            total_tokens += self.count_tokens(item["text"])
                        elif item.get("type") == "image_url" and item.get("image_url", {}).get("url"):
                            # å›¾åƒURLçš„tokenä¼°ç®—ï¼ˆOpenAIçš„ä¼°ç®—æ–¹å¼ï¼‰
                            # åŸºç¡€token + å›¾åƒå¤§å°ä¼°ç®—
                            total_tokens += 85  # åŸºç¡€å›¾åƒtoken
                            
                            # å°è¯•ä»base64ä¼°ç®—å›¾åƒå¤§å°
                            url = item["image_url"]["url"]
                            if url.startswith("data:image"):
                                try:
                                    # æå–base64éƒ¨åˆ†
                                    base64_part = url.split(",")[1]
                                    # ä¼°ç®—tokenï¼ˆæ¯åƒç´ çº¦0.002 tokensï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                                    image_size = len(base64_part) * 0.75  # base64è§£ç åçš„å¤§å°
                                    if image_size > 100000:  # å¤§äº100KB
                                        total_tokens += 170  # é«˜åˆ†è¾¨ç‡å›¾åƒ
                                    elif image_size > 50000:  # å¤§äº50KB
                                        total_tokens += 85   # ä¸­ç­‰åˆ†è¾¨ç‡å›¾åƒ
                                except:
                                    total_tokens += 85  # é»˜è®¤ä¼°ç®—
        
        # è®¡ç®—thinkingçš„token
        if message.get("thinking"):
            total_tokens += self.count_tokens(str(message["thinking"]))
        
        # è®¡ç®—tool_callsçš„token
        if message.get("tool_calls"):
            for tool_call in message["tool_calls"]:
                if "function" in tool_call:
                    func_name = tool_call["function"].get("name", "")
                    func_args = tool_call["function"].get("arguments", "")
                    total_tokens += self.count_tokens(func_name) + self.count_tokens(func_args)
        
        return total_tokens

    def count_tools_tokens(self, tools: list) -> int:
        """è®¡ç®—å·¥å…·å®šä¹‰çš„tokenæ•°é‡"""
        if not tools:
            return 0
        
        # å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²è¿›è¡Œtokenè®¡ç®—
        tools_json = json.dumps(tools, ensure_ascii=False, separators=(',', ':'))
        return self.count_tokens(tools_json)

    def set_initial_tokens(self, system_prompt: str, tools: list):
        """è®¾ç½®ç³»ç»Ÿåˆå§‹tokenï¼ˆç³»ç»Ÿæç¤º+å·¥å…·å®šä¹‰ï¼‰"""
        system_tokens = self.count_tokens(system_prompt)
        tools_tokens = self.count_tools_tokens(tools)
        
        self.initial_tokens["system_prompt_tokens"] = system_tokens
        self.initial_tokens["tools_definition_tokens"] = tools_tokens
        self.initial_tokens["total_initial_tokens"] = system_tokens + tools_tokens
        
        # å°†åˆå§‹tokenè®¡å…¥æ€»è¾“å…¥token
        self.total_stats["total_input_tokens"] = system_tokens + tools_tokens
        self.total_stats["total_tokens"] = system_tokens + tools_tokens
    
    def start_new_round(self):
        """å¼€å§‹æ–°ä¸€è½®å¯¹è¯ï¼Œé‡ç½®å½“å‰è½®æ¬¡ç»Ÿè®¡"""
        self.current_round_stats = {
            "user_input_tokens": 0,
            "llm_output_tokens": 0,
            "tool_result_tokens": 0,
            "total_round_tokens": 0
        }
    
    def calculate_conversation_tokens(self, messages: list) -> int:
        """è®¡ç®—æ•´ä¸ªå¯¹è¯å†å²çš„tokenæ€»æ•°ï¼ˆç”¨äºAPIè°ƒç”¨å‰çš„è¾“å…¥tokenè®¡ç®—ï¼‰"""
        total_tokens = 0
        
        # åŒ…æ‹¬ç³»ç»Ÿæç¤ºå’Œå·¥å…·å®šä¹‰çš„åˆå§‹token
        total_tokens += self.initial_tokens["total_initial_tokens"]
        
        # è®¡ç®—æ‰€æœ‰å†å²æ¶ˆæ¯çš„token
        for message in messages:
            total_tokens += self.count_message_tokens(message)
            
        return total_tokens

    def update_total_stats(self, messages: list):
        """æ ¹æ®å½“å‰å¯¹è¯å†å²æ›´æ–°æ€»tokenç»Ÿè®¡"""
        # è®¡ç®—çœŸå®çš„æ€»è¾“å…¥tokenï¼ˆåŒ…æ‹¬å†å²å¯¹è¯ï¼‰
        actual_input_tokens = self.calculate_conversation_tokens(messages)
        
        # æ›´æ–°æ€»ç»Ÿè®¡
        self.total_stats["total_input_tokens"] = actual_input_tokens
        self.total_stats["total_tokens"] = actual_input_tokens + self.total_stats["total_output_tokens"]

    def add_user_input(self, text: str):
        """æ·»åŠ ç”¨æˆ·è¾“å…¥çš„tokenç»Ÿè®¡ï¼ˆä»…å½“å‰è½®æ¬¡ï¼‰"""
        tokens = self.count_tokens(text)
        self.current_round_stats["user_input_tokens"] = tokens
        # æ³¨æ„ï¼šæ€»è¾“å…¥tokenç”±å¤–éƒ¨calculate_conversation_tokensæ–¹æ³•ç»Ÿä¸€è®¡ç®—

    def add_llm_output(self, thinking: str = "", content: str = "", tool_calls: list = None):
        """æ·»åŠ LLMè¾“å‡ºçš„tokenç»Ÿè®¡"""
        tokens = 0
        
        # æ€ç»´é“¾token
        if thinking:
            tokens += self.count_tokens(thinking)
        
        # è‡ªç„¶è¯­è¨€å†…å®¹token
        if content:
            tokens += self.count_tokens(content)
        
        # å·¥å…·è°ƒç”¨token
        if tool_calls:
            for tool_call in tool_calls:
                if "function" in tool_call:
                    func_name = tool_call["function"].get("name", "")
                    func_args = tool_call["function"].get("arguments", "")
                    tokens += self.count_tokens(func_name) + self.count_tokens(func_args)
        
        self.current_round_stats["llm_output_tokens"] += tokens
        self.total_stats["total_output_tokens"] += tokens

    def add_tool_result(self, result: str):
        """æ·»åŠ å·¥å…·è¿”å›ç»“æœçš„tokenç»Ÿè®¡ï¼ˆä»…å½“å‰è½®æ¬¡ï¼‰"""
        tokens = self.count_tokens(str(result))
        self.current_round_stats["tool_result_tokens"] += tokens
        # æ³¨æ„ï¼šæ€»è¾“å…¥tokenç”±å¤–éƒ¨calculate_conversation_tokensæ–¹æ³•ç»Ÿä¸€è®¡ç®—

    def finish_round(self):
        """å®Œæˆå½“å‰è½®æ¬¡ï¼Œè®¡ç®—æ€»tokenæ•°"""
        self.current_round_stats["total_round_tokens"] = (
            self.current_round_stats["user_input_tokens"] +
            self.current_round_stats["llm_output_tokens"] +
            self.current_round_stats["tool_result_tokens"]
        )

    def get_round_summary(self) -> str:
        """è·å–å½“å‰è½®æ¬¡çš„tokenç»Ÿè®¡æ‘˜è¦"""
        return f"""
ğŸ“Š å½“å‰è½®æ¬¡Tokenç»Ÿè®¡:
   ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {self.current_round_stats['user_input_tokens']} tokens
   ğŸ¤– LLMè¾“å‡º: {self.current_round_stats['llm_output_tokens']} tokens
   ğŸ”§ å·¥å…·ç»“æœ: {self.current_round_stats['tool_result_tokens']} tokens
   ğŸ“ˆ æœ¬è½®æ€»è®¡: {self.current_round_stats['total_round_tokens']} tokens

ğŸ“Š ç´¯è®¡Tokenç»Ÿè®¡:
   ğŸ“ ç³»ç»Ÿåˆå§‹: {self.initial_tokens['total_initial_tokens']} tokens
   ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {self.current_round_stats['user_input_tokens']} tokens (æœ¬è½®)
   ğŸ¤– LLMè¾“å‡º: {self.current_round_stats['llm_output_tokens']} tokens (æœ¬è½®)
   ğŸ”§ å·¥å…·ç»“æœ: {self.current_round_stats['tool_result_tokens']} tokens (æœ¬è½®)
   ğŸ“¥ æ€»è¾“å…¥: {self.total_stats['total_input_tokens']} tokens
   ğŸ“¤ æ€»è¾“å‡º: {self.total_stats['total_output_tokens']} tokens
   ğŸ“Š æ€»è®¡: {self.total_stats['total_tokens']} tokens
"""

    def get_current_operation_summary(self, operation_type: str) -> str:
        """è·å–å½“å‰æ“ä½œçš„tokenç»Ÿè®¡"""
        if operation_type == "user_input":
            return f"ğŸ“¥ ç”¨æˆ·è¾“å…¥: {self.current_round_stats['user_input_tokens']} tokens"
        elif operation_type == "llm_output":
            return f"ğŸ“¤ LLMè¾“å‡º: {self.current_round_stats['llm_output_tokens']} tokens"
        elif operation_type == "tool_result":
            return f"ğŸ“¥ å·¥å…·è¿”å›: {self.current_round_stats['tool_result_tokens']} tokens"
        else:
            return ""

    def get_total_summary(self) -> str:
        """è·å–æ€»tokenç»Ÿè®¡"""
        return f"ğŸ“Š æ€»è¾“å…¥: {self.total_stats['total_input_tokens']} tokens, æ€»è¾“å‡º: {self.total_stats['total_output_tokens']} tokens, æ€»è®¡: {self.total_stats['total_tokens']} tokens"