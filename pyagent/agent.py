from .frontends import FrontendInterface
from .tools import TOOL_FUNCTIONS, TOOLS
from .token_counter import TokenCounter
from .frontends.image_handler import ImageHandler
from . import conversation_saver
from .context_compressor import ContextCompressor
from .llm_adapter import UnifiedLLMClient
import json_repair


class Agent:
    def __init__(
        self, 
        client: UnifiedLLMClient, 
        frontend: FrontendInterface, 
        system_prompt: str, 
        model_name: str,
        model_parameters: list = None
    ):
        self.client = client
        self.frontend = frontend
        self.messages = [{"role": "system", "content": system_prompt}]
        self.model_name = model_name
        self.model_parameters = model_parameters or []
        self.token_counter = TokenCounter(model_name)
        self.context_compressor = ContextCompressor(keep_recent_rounds=2)
        
        # è®¾ç½®ç³»ç»Ÿåˆå§‹tokenï¼ˆç³»ç»Ÿæç¤º+å·¥å…·å®šä¹‰ï¼‰
        from .tools import TOOLS
        self.token_counter.set_initial_tokens(system_prompt, TOOLS)


    def run(self):
        try:
            self.frontend.start_session()
            total_input_tokens = 0
            total_output_tokens = 0
            while True:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input, has_input = self.frontend.get_input()
                if not has_input or user_input.lower() == 'é€€å‡º':
                    break
                
                # å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œæå–å›¾åƒ
                clean_text, content_parts = ImageHandler.process_user_input(user_input)
                
                # ç»Ÿè®¡å›¾åƒæ•°é‡
                image_count = len([part for part in content_parts if part.get("type") == "image_url"])
                
                # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯ä¸Šä¸‹æ–‡
                if content_parts:
                    # ä½¿ç”¨contentåˆ—è¡¨æ ¼å¼ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
                    self.messages.append({"role": "user", "content": content_parts})
                else:
                    # å›é€€åˆ°çº¯æ–‡æœ¬æ ¼å¼
                    self.messages.append({"role": "user", "content": clean_text or "è¯·åˆ†æä¸Šä¼ çš„å›¾åƒ"})
                
                conversation_saver.save_conversation(self.messages)

                # è®¡ç®—è¾“å…¥tokenæ€»æ•°
                user_tokens = self.token_counter.count_tokens(clean_text)
                image_info = f" å·²æ·»åŠ å›¾åƒ: {image_count}å¼ " if image_count > 0 else ""
                self.frontend.output('info', f"ğŸ“Š ç”¨æˆ·è¾“å…¥: {user_tokens} tokens{image_info}")
                tool_result_tokens = 0
                while True:
                    full_response = ""  # LLMè‡ªç„¶è¯­è¨€è¾“å‡º
                    tool_calls_cache = {}  # å·¥å…·è°ƒç”¨ç¼“å­˜
                    reasoning_content = ""  # LLMæ€è€ƒè¿‡ç¨‹
                    # çŠ¶æ€æ ‡å¿—
                    has_received_reasoning = False

                    # å‹ç¼©ä¸Šä¸‹æ–‡ä»¥èŠ‚çœtoken
                    compressed_messages = self.context_compressor.compress_context(self.messages)

                    # è®¡ç®—æœ¬æ¬¡è¯·æ±‚çš„ä¸Šä¸‹æ–‡çª—å£tokené‡
                    context_window_tokens = self.token_counter.calculate_conversation_tokens(compressed_messages)
                    total_input_tokens += context_window_tokens
                    self.frontend.output('info', f"ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: {context_window_tokens/1000} åƒtokens ğŸ“Š è¾“å…¥tokenæ€»é‡: {total_input_tokens} tokens  ğŸ“Š è¾“å‡ºtokenæ€»é‡: {total_output_tokens} tokens")

                    # è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯
                    stats = self.context_compressor.get_compression_stats(self.messages, compressed_messages)
                    if stats["saved_chars"] > 0:
                        self.frontend.output('info', f"ä¸Šä¸‹æ–‡å‹ç¼©: èŠ‚çœ {stats['saved_chars']} å­—ç¬¦ ({stats['compression_ratio']}%)")

                    # æ„å»ºé»˜è®¤å‚æ•°
                    api_params = {
                        "model": self.model_name,
                        "messages": compressed_messages,
                        "stream": True,
                        "tools": TOOLS,
                        "max_tokens": 32000,
                    }
                    
                    # åº”ç”¨æ¨¡å‹å‚æ•°
                    for param in self.model_parameters:
                        if isinstance(param, list) and len(param) == 2:
                            key, value = param
                            # å¦‚æœå‚æ•°å€¼ä¸º"Delete"ï¼Œåˆ™ä»api_paramsä¸­ç§»é™¤è¯¥å‚æ•°
                            if value == "Delete":
                                if key in api_params:
                                    del api_params[key]
                            else:
                                api_params[key] = value
                    
                    # è°ƒç”¨LLMç”Ÿæˆå“åº”ï¼ˆæµå¼ï¼‰
                    stream = self.client.chat_completions_create(**api_params)

                    finish_reason = None
                    for chunk in stream:
                        # è·å–finish_reason
                        if hasattr(chunk.choices[0], 'finish_reason') and chunk.choices[0].finish_reason:
                            finish_reason = chunk.choices[0].finish_reason

                        # å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆæ€ç»´é“¾ï¼‰
                        if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                            if not has_received_reasoning:
                                has_received_reasoning = True
                            reasoning_content += chunk.choices[0].delta.reasoning_content
                            self.frontend.output('thinking', chunk.choices[0].delta.reasoning_content)

                        # å¤„ç†è‡ªç„¶è¯­è¨€å†…å®¹
                        elif chunk.choices[0].delta.content:
                            # æ— è®ºæ˜¯å¦å¤„äºæ€è€ƒæ¨¡å¼éƒ½ç›´æ¥è¾“å‡ºå†…å®¹
                            chunk_content = chunk.choices[0].delta.content
                            full_response += chunk_content
                            self.frontend.output('content', chunk_content)

                        # å¤„ç†å·¥å…·è°ƒç”¨
                        tool_calls = getattr(chunk.choices[0].delta, 'tool_calls', None)
                        if tool_calls is not None:
                            for tool_chunk in tool_calls if tool_calls else []:
                                if tool_chunk.index not in tool_calls_cache:
                                    tool_calls_cache[tool_chunk.index] = {
                                        'id': '',
                                        'function': {'name': '', 'arguments': ''}
                                    }
                                    # é¦–æ¬¡æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æ—¶æç¤º
                                    if hasattr(tool_chunk.function, 'name') and tool_chunk.function.name:
                                        self.frontend.output('tool_call', tool_chunk.function.name)

                                # æ›´æ–°å·¥å…·IDã€åç§°ã€å‚æ•°
                                if tool_chunk.id:
                                    tool_calls_cache[tool_chunk.index]['id'] = tool_chunk.id
                                if hasattr(tool_chunk.function, 'name') and tool_chunk.function.name:
                                    tool_calls_cache[tool_chunk.index]['function']['name'] = tool_chunk.function.name
                                if hasattr(tool_chunk.function, 'arguments') and tool_chunk.function.arguments:
                                    tool_calls_cache[tool_chunk.index]['function']['arguments'] += tool_chunk.function.arguments
                                    # æ˜¾ç¤ºå‚æ•°æ¥æ”¶è¿›åº¦ï¼ˆæ¯50å­—ç¬¦ä¸€ä¸ªç‚¹ï¼‰
                                    if len(tool_calls_cache[tool_chunk.index]['function']['arguments']) % 50 == 0:
                                        self.frontend.output('tool_progress', ".")

                    # è¾“å‡ºç»“æŸä¿¡æ¯
                    if finish_reason:
                        self.frontend.output('end', f"\n[Streamç»“æŸ] å®ŒæˆåŸå› : {finish_reason}")

                    # å¤„ç†è‡ªç„¶è¯­è¨€è¾“å‡ºï¼ˆè‹¥æœ‰ï¼‰
                    if full_response:
                        self.messages.append({
                            "role": "assistant",
                            "content": full_response,
                            "thinking": reasoning_content  # ä¿å­˜æ€è€ƒè¿‡ç¨‹
                        })
                        conversation_saver.save_conversation(self.messages)
                        
                        # è®¡ç®—LLMè¾“å‡ºçš„tokenæ•°é‡
                        reasoning_tokens = 0
                        response_tokens = 0
                        if reasoning_content:
                            reasoning_tokens = self.token_counter.count_tokens(reasoning_content)
                            total_output_tokens += reasoning_tokens
                        if full_response:
                            response_tokens = self.token_counter.count_tokens(full_response)
                            total_output_tokens += response_tokens
                        
                        # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥åçš„tokenç»Ÿè®¡ + LLMè¾“å‡ºåçš„tokenç»Ÿè®¡
                        self.frontend.output('info', f"ğŸ“Š æ€è€ƒè¾“å‡º: {reasoning_tokens} tokens  ğŸ“Š å›ç­”è¾“å‡º: {response_tokens} tokens")
                        self.frontend.output('info', f"ğŸ“Š è¾“å…¥tokenæ€»é‡: {total_input_tokens} tokens  ğŸ“Š è¾“å‡ºtokenæ€»é‡: {total_output_tokens} tokens")

                    # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆè‹¥æœ‰ï¼‰
                    if tool_calls_cache:
                        self.frontend.output('info', "\nå·¥å…·å‚æ•°æ¥æ”¶å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œ...")
                        
                        # è®¡ç®—å·¥å…·è°ƒç”¨çš„token
                        tool_calls_tokens = 0
                        tool_calls_list = [
                            {
                                'id': tool_call['id'],
                                'type': 'function',
                                'function': {
                                    'name': tool_call['function']['name'],
                                    'arguments': tool_call['function']['arguments']
                                }
                            } for tool_call in tool_calls_cache.values()
                        ]
                        
                        for tool_call in tool_calls_list:
                            if "function" in tool_call:
                                func_name = tool_call["function"].get("name", "")
                                func_args = tool_call["function"].get("arguments", "")
                                tool_calls_tokens += self.token_counter.count_tokens(func_name) + self.token_counter.count_tokens(func_args)
                        
                        # è®¡ç®—è¾“å‡ºtokenæ€»æ•°ï¼šå·¥å…·è°ƒç”¨token + ä¹‹å‰çš„è¾“å…¥token
                        total_output_tokens += tool_calls_tokens
                        self.frontend.output('info', f"ğŸ“Š è°ƒç”¨è¯·æ±‚è¾“å‡ºtokené‡: {tool_calls_tokens}")

                        # æ·»åŠ å·¥å…·è°ƒç”¨æŒ‡ä»¤åˆ°å¯¹è¯ä¸Šä¸‹æ–‡
                        self.messages.append({
                            "role": "assistant",
                            "content": None,
                            "tool_calls": tool_calls_list,
                            "thinking": reasoning_content  # ä¿å­˜æ€è€ƒè¿‡ç¨‹
                        })
                        conversation_saver.save_conversation(self.messages)

                        # æ‰§è¡Œå·¥å…·å¹¶è®°å½•ç»“æœ
                        for tool_call in tool_calls_cache.values():
                            function_name = tool_call['function']['name']
                            try:
                                function_args = json_repair.loads(tool_call['function']['arguments'])
                            except Exception as e:
                                self.frontend.output("error", f"å·¥å…·å‚æ•°è§£æå¤±è´¥ï¼š{tool_call['function']['arguments']} - {str(e)}")
                                continue

                            if function_name in TOOL_FUNCTIONS:
                                try:
                                    function_response = TOOL_FUNCTIONS[function_name](**function_args)
                                    # æ·»åŠ å·¥å…·è¿”å›ç»“æœåˆ°å¯¹è¯ä¸Šä¸‹æ–‡
                                    self.messages.append({
                                        "role": "tool",
                                        "content": str(function_response),
                                        "tool_call_id": tool_call['id']
                                    })
                                    conversation_saver.save_conversation(self.messages)
                                    
                                    # è®¡ç®—å·¥å…·è¿”å›ç»“æœçš„token
                                    tool_result_tokens = self.token_counter.count_tokens(str(function_response))
                                    self.frontend.output("tool_result",f"{function_response}")
                                    self.frontend.output('info', f"ğŸ“Š å·¥å…·è¿”å›tokené‡: {tool_result_tokens}")

                                except Exception as e:
                                    self.frontend.output('error', f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{function_name} - {str(e)}")
                            else:
                                self.frontend.output('error', f"âŒ æœªæ‰¾åˆ°å·¥å…·å‡½æ•°ï¼š{function_name}")
                    else:
                        # æ— å·¥å…·è°ƒç”¨æ—¶ç»“æŸå½“å‰è½®æ¬¡
                        break

        except Exception as e:
            self.frontend.output('error', f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        finally:
            # ç¨‹åºç»“æŸæ—¶æ¢å¤ç»ˆç«¯é¢œè‰²ä¸ºé»˜è®¤å€¼
            self.frontend.end_session()
            # ç¨‹åºç»“æŸæ—¶ä¿å­˜æœ€åä¸€æ¬¡å¯¹è¯
            conversation_saver.save_conversation(self.messages)
