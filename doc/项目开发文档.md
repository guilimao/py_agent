### 项目开发文档

---

#### 一、项目整体结构
项目主要包含核心业务模块、配置管理、工具集、输入处理及对话存储等部分，结构如下：
```
项目根目录
├── .git/                   # Git版本控制目录
├── .gitignore              # Git忽略规则文件
├── agent.py                # 核心对话代理类（管理对话流程、工具调用）
├── config/                 # 配置管理目录
│   ├── provider_config.json# 模型提供商配置（API密钥、模型列表等）
│   └── __init__.py         # （隐式）配置模块入口（如`get_system_prompt`函数）
├── conversation_memory.json# 对话历史存储文件（JSON格式）（文件较大，不宜直接阅读其内容）
├── input_handler.py        # 用户输入处理模块（获取输入）
├── main.py                 # 项目入口文件（参数解析、初始化、运行代理）
├── README.md               # 中文项目说明
├── README_EN.md            # 英文项目说明
├── system_prompt.json      # 系统提示词配置文件（JSON格式）
├── test/                   # 测试用例目录（待填充）
└── tools/                  # 工具集模块
    ├── __init__.py         # 工具整合入口（导出工具描述和函数）
    ├── file_op.py          # （隐式）文件操作工具（如read_file、create_file等）
    └── cmdline.py          # （隐式）命令行工具（如execute_command）
```

---

#### 二、核心模块详解

##### 1. `agent.py`：对话代理核心类
**作用**：负责与大语言模型（LLM）的交互流程管理，包括对话历史保存、工具调用、流式响应处理。

**关键类与方法**：
- **类 `Agent`**  
  - **初始化方法 `__init__`**  
    接收OpenAI客户端实例、用户输入获取函数、系统提示词和模型名称，初始化对话上下文（`messages`）。  
    参数：`client`（OpenAI客户端）、`get_user_message`（输入函数）、`system_prompt`（系统提示）、`model_name`（LLM模型名）。

  - **方法 `save_conversation`**  
    将对话历史（用户输入与AI响应）追加保存到 `conversation_memory.json`。  
    逻辑：读取现有对话 → 合并新对话 → 覆盖写入文件。

  - **方法 `run`**  
    主对话循环：  
    1. 持续获取用户输入（通过 `get_user_message`）；  
    2. 调用LLM生成响应（支持流式输出和工具调用）；  
    3. 处理工具调用（调用`tools`模块中的工具函数）；  
    4. 异常处理与最终对话保存。

**依赖**：  
- `openai` 库（与LLM交互）；  
- `tools` 模块（`TOOLS`工具描述、`TOOL_FUNCTIONS`工具函数映射）；  
- `input_handler.py`（`get_user_message`函数）；  
- `conversation_memory.json`（对话存储）。

---

##### 2. `input_handler.py`：用户输入处理
**作用**：封装用户输入获取逻辑，处理输入异常（如EOF、键盘中断）。

**关键函数**：
- `get_user_message() -> Tuple[str, bool]`  
  从标准输入读取用户输入，返回输入内容和是否成功获取的布尔值。  
  异常处理：捕获`EOFError`和`KeyboardInterrupt`，返回空内容和`False`。

**依赖**：`typing.Tuple`（类型提示）。

---

##### 3. `main.py`：项目入口
**作用**：解析命令行参数，初始化配置和客户端，启动对话代理。

**关键函数与流程**：
- `load_provider_config()`：读取`config/provider_config.json`，获取模型提供商配置（API密钥环境变量、基础URL、支持模型列表）。  
- `get_provider_from_model(model_name, provider_config)`：根据模型名查找对应的提供商。  
- `main()`：  
  1. 解析命令行参数（指定LLM模型）；  
  2. 加载提供商配置，验证模型有效性；  
  3. 初始化OpenAI客户端（使用环境变量中的API密钥）；  
  4. 创建`Agent`实例（传入客户端、输入函数、系统提示）；  
  5. 运行代理（`agent.run()`）。

**依赖**：  
- `argparse`（参数解析）；  
- `os`（读取环境变量）；  
- `json`（配置文件解析）；  
- `openai` 库（客户端初始化）；  
- `agent.py`（`Agent`类）；  
- `config`模块（`get_system_prompt`函数）；  
- `input_handler.py`（`get_user_message`函数）。

---

##### 4. `tools/__init__.py`：工具集整合
**作用**：合并文件操作工具（`file_op.py`）和命令行工具（`cmdline.py`）的描述与函数，供`agent.py`调用。

**关键导出**：
- `TOOLS`：工具描述列表（LLM可识别的工具元数据）；  
- `TOOL_FUNCTIONS`：工具名称到函数的映射（实际执行工具逻辑）。

**依赖**：  
- 内部子模块`file_op`（文件操作工具）和`cmdline`（命令行工具）。

---

##### 5. 配置与存储文件
- **`config/provider_config.json`**：定义模型提供商信息，格式示例：
  ```json
  {
    "provider1": {
      "api_key_env": "OPENAI_API_KEY",
      "base_url": "https://api.openai.com/v1",
      "models": ["doubao-1-5-thinking-pro-250415", "gpt-3.5-turbo"]
    }
  }
  ```
- **`system_prompt.json`**：存储系统提示词（控制LLM行为的初始指令）。  
- **`conversation_memory.json`**：对话历史存储文件，格式为：
  ```json
  {
    "conversations": [
      {"user_input": "你好", "ai_response": "你好！有什么可以帮你？"}
    ]
  }
  ```

---

#### 三、模块依赖关系图
```
main.py → agent.py → tools/__init__.py → file_op.py/cmdline.py
       ↓          ↓
input_handler.py  conversation_memory.json
       ↓
config/provider_config.json/system_prompt.json
```

---

#### 四、补充说明
- **测试目录 `test/`**：建议添加单元测试（如`input_handler.py`的输入测试、`Agent.save_conversation`的文件读写测试）。  
- **扩展工具**：新增工具时，需在`tools/file_op.py`或`cmdline.py`中定义函数，并更新`TOOLS`和`TOOL_FUNCTIONS`。  
- **配置管理**：敏感信息（如API密钥）通过环境变量读取（`provider_config.json`中指定`api_key_env`），避免硬编码。